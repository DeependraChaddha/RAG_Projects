{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzhTDKetbknlAy4g1eeQAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeependraChaddha/RAG_Projects/blob/main/RAG_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, an environment will be set up then different method of query translation will be demnostrated. The methods to be deomnstrated are Multi-Query, RAG-Fusion, Decomposition, Step Back and HyDE"
      ],
      "metadata": {
        "id": "a4ZXDuINUz1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up Environment"
      ],
      "metadata": {
        "id": "YiqTqIwMVShJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Packages"
      ],
      "metadata": {
        "id": "c8jz5Q-BVZok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zgVNcB6TrKC"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Langsmith"
      ],
      "metadata": {
        "id": "4IcVnc2qWDHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "os.environ['LANGCHAIN_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY']=###YOUR OWN API KEY###"
      ],
      "metadata": {
        "id": "JB2H-rp2WFvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI api key"
      ],
      "metadata": {
        "id": "1gsra6fbWuyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=###your api key###"
      ],
      "metadata": {
        "id": "7zSgDc8jX_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Query"
      ],
      "metadata": {
        "id": "_5CaLqC1YLSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "c5t7CUCZY7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### INDEXING #####\n",
        "#Loading document/ blog\n",
        "\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader=WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),       #link of blog\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\",\"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")###Made a WebBaseLoader instance in this line\n",
        "blog_docs=loader.load()#Used the WebBaseLoader instance to load the documents\n",
        "\n",
        "#SPLIT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")# made an instance of RecursiveCharacterTextSplitter\n",
        "###Make the Split###\n",
        "splits=text_splitter.split_documents(blog_docs)\n",
        "\n",
        "#Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore= Chroma.from_documents(documents=splits,\n",
        "                                   embedding=OpenAIEmbeddings())#Stored the split documents in Chroma vector database using OpenAIEmbeddings\n",
        "retriever=vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "wQMlx-FRYOp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "vO_-tlLcfCiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#Multi-Query:Different Perspectives\n",
        "template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#Now the prompt will be broken down to generate multiple independent queries then these queries independently make the RAG retrieve documents, this way a more wide search can be done inside the documents\n",
        "from langchain_core.output_parsers import StrOutputParsers\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(\n",
        "    prompt_perspectives\n",
        "    | ChaOpenAI(temperature=0)\n",
        "    |StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")##This generates a list of queries"
      ],
      "metadata": {
        "id": "ENeLrRvafESU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "  \"\"\"This function makes a unique union of all the retrieved doucments\"\"\"\n",
        "  #Flatten list of lists, convert each document to string\n",
        "  flattened_docs=[dumps{docs} for sublist in documents for doc  in sublist]\n",
        "  #getting unique documents\n",
        "  unique_documents=list(set(flattened_docs))\n",
        "  return [loads(doc) for doc in unique_docs]\n",
        "#retrieve\n",
        "question= \"What is Task Decompositon for LLM agents?\"\n",
        "retrieval_chain=generate_queries |retriever.map()|get_unique_union #this gives the entire chain, first generate quesries then retrieves the different answers then get the unique union of all the documents retrieved\n",
        "docs= retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)\n"
      ],
      "metadata": {
        "id": "kfwdxkZ9hZUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "#RAG\n",
        "template= \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm=ChatOpenAI(temperature =0)\n",
        "\n",
        "#making final rag chain\n",
        "final_rag_chain=(\n",
        "    {\"context\":retrieval_chain,\n",
        "     \"question\":itemgetter(\"question\")}\n",
        "    |prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ")\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "mkR3088IKayu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Fusion"
      ],
      "metadata": {
        "id": "q62l9iXhic6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is mostly similar to Multi-Query, except that after retrieveing multiple documents, the documents are reranked and given a score."
      ],
      "metadata": {
        "id": "1F-8FCumiiCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "AFRmDaT_j3aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#RAG-Fusion: Related\n",
        "template=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion= ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "bNhhzS-gifIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(prompt_rag_fusion\n",
        "                  | ChatOpenAI(temperature=0)\n",
        "                  |StrOutputParser()\n",
        "                  |(lambda x: x.split(\"\\n\"))\n",
        "                  )"
      ],
      "metadata": {
        "id": "sVH-yKLAleSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60): #Takes multiple documents and k value used in RRF formula\n",
        "  #1. Make a dicionary to stores fused scores of each document\n",
        "  fused_scores={}\n",
        "\n",
        "  #2. Iterate through each list of documents\n",
        "  for docs in results:\n",
        "    #2.1. iterating through each document in a list\n",
        "    for rank, doc in enumerate(docs):\n",
        "      #2.1.1. convert each doc to string (assuming docs can be serialized to JSON)\n",
        "      doc_str=dumps(doc)\n",
        "      #2.1.2. check if doc_str in not already present in fused_scores(to avoid repitition), if its not, then add with initia score=0\n",
        "      if doc_str not in fused_scores:\n",
        "        fused_scores[doc_str]=0\n",
        "      #2.1.3. Retrieve the current score of the document(whether already present or just added)\n",
        "      previous_score=fused_scores[doc_str]\n",
        "      #2.1.4. Update score using RRF formula\n",
        "      fused_scores+= 1/(rank + k)\n",
        "  #3. Sort the documents based on rank in descending order\n",
        "  reranked_results=[(loads(doc),score) for doc,score in sorted(fusion_scores.items().key= lambda x:x[1], reverse= True)]#Sorts fusion_scores dictionary according to score which is mentioned as the key and loads the doc and score into a list of tuples\n",
        "\n",
        "  #4. Return the raranked list\n",
        "  return reranked_results\n",
        "\n",
        "#make chain\n",
        "retrieval_chain_rag_fusion= generate_queries | retriever.map() |reciprocal_rank_fusion\n",
        "docs= retrieval_chain_rag_fusion.invoke({\"question\":question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "WLQknhmsmUNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassThrough\n",
        "\n",
        "#RAG\n",
        "template= \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain= (\n",
        "    {\"context\":retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "finalrag_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "XQfCd-Wj0k5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decomposition"
      ],
      "metadata": {
        "id": "oxuK3OYeMHPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach breaks down the query into smaller parts then retrieval is firstly done on smaller, simpler queries then the generated answer is also given as prompt with the actual question to get a better answer generated."
      ],
      "metadata": {
        "id": "3wyoViSJMKqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#Decomposition\n",
        "template=\"\"\"ou are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition=ChatPromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "PLqZLzDYMkP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#LLM\n",
        "llm= ChatOpenAI(temperature=0) #This fixes the llm we use\n",
        "\n",
        "#Chain\n",
        "generate_queries_decomposition= (prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "#Run\n",
        "question=\"What are the main components of an LLM-powered autonomous agent system?\"\n",
        "questions=generate_queries_decomposition.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "8Z9ExioENBKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "cRbkVCBqQUyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 ways of using the answers to the decomposed questions\n",
        "a) Answer Recursively\n",
        "b) Answer Individually"
      ],
      "metadata": {
        "id": "4V7i_L3hQckQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Recursively"
      ],
      "metadata": {
        "id": "3X7dFRcSTOve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template= \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "xogeRE3WQtqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "\n",
        "  #initialize formatted string\n",
        "  formatted_string=\"\"\n",
        "  formatted_string += f\"Question:{question} \\n Answer:{answer}\\n\\n\"\n",
        "  return formatted_string.strip()\n",
        "\n",
        "#specify llm\n",
        "llm= ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "q_a_pairs=\"\"\n",
        "\n",
        "for q in questions:\n",
        "\n",
        "  rag_chain=(\n",
        "      {\"context\":itemgetter(\"question\")| retriever,\n",
        "       \"question\":itemgetter(\"question\"),\n",
        "       \"q_a_pairs\":itemgetter(\"q_a_pairs\")} #this dictionary gives previous context, questions and q_a_pairs to the decomposition_prompt function\n",
        "      |decomposition_prompt # this decomposes the prompt\n",
        "      |llm # input the prompt in llm\n",
        "      |StrOutputParser())#this parses the output of llm\n",
        "  answer=rag_chain.invoke({\"question\":q, \"q_a_pairs\":q_a_pairs})\n",
        "  q_a_pair=format_qa_pair(q,answer)\n",
        "  q_a_pairs=q_a_pairs + \"\\n---\\n\" + q_a_pair"
      ],
      "metadata": {
        "id": "sJxRRiYJbyF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "vANZh5JogC8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Individually"
      ],
      "metadata": {
        "id": "t4K-y0pWijDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answering each question individually\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassThrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#RAG Prompt\n",
        "prompt= hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def retieve_and_rag(question, prompt, sub_question_generator_chain):\n",
        "  ###This function uses the rag on each sub-question###\n",
        "\n",
        "  #1. Use the decomposition\n",
        "  sub_questions=sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "  #2. Store RAG chain results\n",
        "  #2.1. Initiialize list to store results\n",
        "  rag_results=[]\n",
        "\n",
        "  for sub_question in sub_questions:\n",
        "    # 2.1.1 Retrieve for each sub-question\n",
        "    retrieved_docs=retriever.get_relevant_documents(sub_question)\n",
        "\n",
        "    #2.1.2 Use sub_questions and retrieved documents in RAG chain\n",
        "    answer= (prompt_rag |llm |StrOutputParser()).invoke({\"context\":retrieved_docs,\n",
        "                                                         \"question\":sub_question})\n",
        "    #2.1.3 Append answer in the list\n",
        "    rag_results.append(answer)\n",
        "\n",
        "  #3. Return the results and the sub-questions\n",
        "  return rag_results, sub_questions\n",
        "\n",
        "#Wrap the returned values into RunnableLambda for Chain usage\n",
        "answers,questions=retrieve_and_rag(question,\n",
        "                                   prompt_rag,\n",
        "                                   generate_queries_decomposition)#We use the same function to make sub-questions in both the approaches"
      ],
      "metadata": {
        "id": "e0v_PxFJilUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make function to format Q & A pairs\n",
        "def format_qa_pairs(questions, answers):\n",
        "\n",
        "  #Initialize formatted string\n",
        "  formatted_string=\"\"\n",
        "  for i,(question, answer) in enumerate(zip(questions,answers), start=1):\n",
        "    formatted_string += f\"Question {i}: {question} \\n Answer {i}: {answer}\\n\\n\"\n",
        "  return formatted_string.strip()\n",
        "\n",
        "context=format_qa_pairs(questions, answers)\n",
        "\n",
        "\n",
        "#Prompt\n",
        "template =\"\"\"ere is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt= ChatPromptTemplate.from_template(template) #This gives the entire prompt generated using all the context retrieved from the sub-questions.\n",
        "\n",
        "final_rag_chain =(\n",
        "    prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ") #Built the entire RAG chain\n",
        "\n",
        "#invoke context and question in the final rag chain\n",
        "final_rag_chain.invoke({\"context\":context, \"question\":question})"
      ],
      "metadata": {
        "id": "-L84VbqqDbb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step Back"
      ],
      "metadata": {
        "id": "ai5dyUCSdj-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This increases the abstraction of the question before asking the actual question to provide better context to the LLM to get a better answer"
      ],
      "metadata": {
        "id": "pfv6DDJElotC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "#Declaring examples of step back questions\n",
        "examples= [{\n",
        "    \"input\":\"Could the members of The Police perform lawful arrests?\"\n",
        "    \"output\":\"what can the members of The Police do?\"\n",
        "},\n",
        "    {\"input\":\"Jan Sindel’s was born in what country?\"\n",
        "    \"output\":\"what is Jan Sindel’s personal history?\"},]\n",
        "\n",
        "#Transform to examine messages\n",
        "example_prompt=ChatPromptTemplate.from_messages(\n",
        "    [(\"human\",\"{input}\"),\n",
        "     (\"ai\",\"{output}\")]\n",
        ")# Made a ChatPromptTemplate object\n",
        "#Now, use this template object to make prompts\n",
        "few_shot_prompt=FewShotPromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "#Making the entire prompt to give to RAG\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        #Examples\n",
        "        few_shot_prompt,\n",
        "        #New Question\n",
        "        (\"user\",\"{question}\"),\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "3jlw7y15dlt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make RAG chain\n",
        "generate_queries_step_back=prompt|ChatOpenAI(temperature=0)| StrOutputParser()\n",
        "question=\"What is task decomposition for LLM agents?\"\n",
        "#Invoke question in chain\n",
        "generate_queries_step_back.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "VEnuIzsgquOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Response Prompt\n",
        "response_prompt_template=\"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "response_prompt= ChatPromptTemplate.from_template(response_prompt_template)\n",
        "\n",
        "chain=(\n",
        "    {#normal context\n",
        "     \"normal_context\":RunnableLambda(lambda x:x[\"question\"])| retriever, #this runnable lamda takes input and find the value associated to kargument \"question\" and pipes it to the retriever\n",
        "     #step-back context\n",
        "     \"step_back_context\":generate_queries_step_back|retriever, #pipes step back questions to the retriever which gives additional context\n",
        "     #Pass on the question\n",
        "     \"question\":lambda x: x[\"question\"],}\n",
        "    |response_prompt\n",
        "    |ChatOpenAI(temperature=0)\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "vnrv2qq_reNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HyDE(Hypothetical Document Embedding)"
      ],
      "metadata": {
        "id": "KbWn7X25X87X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this method, hypothetical documents are generated based on the query made,then the hypothetical documents are embedded with the query in the vector database to give additional context for retrieval  "
      ],
      "metadata": {
        "id": "51-BUedcX-ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make imports\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#HyDE document generation\n",
        "#make template\n",
        "template=\"\"\"Please write a scientific paper passage to answer the question\n",
        "Question: {question}\n",
        "Passage:\"\"\"\n",
        "prompt_hyde=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "#make RAG chain\n",
        "generate_docs_for_retrieval=(\n",
        "    prompt_hyde| ChatOpenAI(temperature=0) |StrOutputParser()\n",
        ")\n",
        "\n",
        "#Run\n",
        "question= \"What is task decomposition for LLM agents?\"\n",
        "generate_docs_for_retrieval.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "TGW2kjAoX_NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve\n",
        "retrieval_chain= generate_docs_for_retrieval | retriever\n",
        "retrieved_docs=retireval_chain.invoke(\"question\":question)\n",
        "retrieved_docs"
      ],
      "metadata": {
        "id": "zGgt6z-0sI-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG\n",
        "template=\"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "#make prompt using template\n",
        "prompt= ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "#make entire rag chain\n",
        "final_rag_chain= (\n",
        "    prompt\n",
        "    |llm\n",
        "    |StrOutputParser\n",
        ")\n",
        "\n",
        "\n",
        "#Use retrieved documents as context and the original question as the question/query\n",
        "final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
      ],
      "metadata": {
        "id": "5N5-AEzNsj5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}