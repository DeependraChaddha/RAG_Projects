{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUf9ccqP9JD8aQ7yAU7Vs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeependraChaddha/RAG_Projects/blob/main/RAG_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, an environment will be set up then different method of query translation will be demnostrated. The methods to be deomnstrated are Multi-Query, RAG-Fusion, Decomposition, Step Back and HyDE"
      ],
      "metadata": {
        "id": "a4ZXDuINUz1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up Environment"
      ],
      "metadata": {
        "id": "YiqTqIwMVShJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Packages"
      ],
      "metadata": {
        "id": "c8jz5Q-BVZok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zgVNcB6TrKC"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Langsmith"
      ],
      "metadata": {
        "id": "4IcVnc2qWDHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "os.environ['LANGCHAIN_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY']=###YOUR OWN API KEY###"
      ],
      "metadata": {
        "id": "JB2H-rp2WFvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI api key"
      ],
      "metadata": {
        "id": "1gsra6fbWuyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=###your api key###"
      ],
      "metadata": {
        "id": "7zSgDc8jX_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Query"
      ],
      "metadata": {
        "id": "_5CaLqC1YLSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "c5t7CUCZY7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### INDEXING #####\n",
        "#Loading document/ blog\n",
        "\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader=WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),       #link of blog\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\",\"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")###Made a WebBaseLoader instance in this line\n",
        "blog_docs=loader.load()#Used the WebBaseLoader instance to load the documents\n",
        "\n",
        "#SPLIT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")# made an instance of RecursiveCharacterTextSplitter\n",
        "###Make the Split###\n",
        "splits=text_splitter.split_documents(blog_docs)\n",
        "\n",
        "#Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore= Chroma.from_documents(documents=splits,\n",
        "                                   embedding=OpenAIEmbeddings())#Stored the split documents in Chroma vector database using OpenAIEmbeddings\n",
        "retriever=vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "wQMlx-FRYOp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "vO_-tlLcfCiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#Multi-Query:Different Perspectives\n",
        "template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#Now the prompt will be broken down to generate multiple independent queries then these queries independently make the RAG retrieve documents, this way a more wide search can be done inside the documents\n",
        "from langchain_core.output_parsers import StrOutputParsers\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(\n",
        "    prompt_perspectives\n",
        "    | ChaOpenAI(temperature=0)\n",
        "    |StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")##This generates a list of queries"
      ],
      "metadata": {
        "id": "ENeLrRvafESU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "  \"\"\"This function makes a unique union of all the retrieved doucments\"\"\"\n",
        "  #Flatten list of lists, convert each document to string\n",
        "  flattened_docs=[dumps{docs} for sublist in documents for doc  in sublist]\n",
        "  #getting unique documents\n",
        "  unique_documents=list(set(flattened_docs))\n",
        "  return [loads(doc) for doc in unique_docs]\n",
        "#retrieve\n",
        "question= \"What is Task Decompositon for LLM agents?\"\n",
        "retrieval_chain=generate_queries |retriever.map()|get_unique_union #this gives the entire chain, first generate quesries then retrieves the different answers then get the unique union of all the documents retrieved\n",
        "docs= retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)\n"
      ],
      "metadata": {
        "id": "kfwdxkZ9hZUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "#RAG\n",
        "template= \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm=ChatOpenAI(temperature =0)\n",
        "\n",
        "#making final rag chain\n",
        "final_rag_chain=(\n",
        "    {\"context\":retrieval_chain,\n",
        "     \"question\":itemgetter(\"question\")}\n",
        "    |prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ")\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "mkR3088IKayu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Fusion"
      ],
      "metadata": {
        "id": "q62l9iXhic6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is mostly similar to Multi-Query, except that after retrieveing multiple documents, the documents are reranked and given a score."
      ],
      "metadata": {
        "id": "1F-8FCumiiCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "AFRmDaT_j3aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#RAG-Fusion: Related\n",
        "template=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion= ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "bNhhzS-gifIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(prompt_rag_fusion\n",
        "                  | ChatOpenAI(temperature=0)\n",
        "                  |StrOutputParser()\n",
        "                  |(lambda x: x.split(\"\\n\"))\n",
        "                  )"
      ],
      "metadata": {
        "id": "sVH-yKLAleSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60): #Takes multiple documents and k value used in RRF formula\n",
        "  #1. Make a dicionary to stores fused scores of each document\n",
        "  fused_scores={}\n",
        "\n",
        "  #2. Iterate through each list of documents\n",
        "  for docs in results:\n",
        "    #2.1. iterating through each document in a list\n",
        "    for rank, doc in enumerate(docs):\n",
        "      #2.1.1. convert each doc to string (assuming docs can be serialized to JSON)\n",
        "      doc_str=dumps(doc)\n",
        "      #2.1.2. check if doc_str in not already present in fused_scores(to avoid repitition), if its not, then add with initia score=0\n",
        "      if doc_str not in fused_scores:\n",
        "        fused_scores[doc_str]=0\n",
        "      #2.1.3. Retrieve the current score of the document(whether already present or just added)\n",
        "      previous_score=fused_scores[doc_str]\n",
        "      #2.1.4. Update score using RRF formula\n",
        "      fused_scores+= 1/(rank + k)\n",
        "  #3. Sort the documents based on rank in descending order\n",
        "  reranked_results=[(loads(doc),score) for doc,score in sorted(fusion_scores.items().key= lambda x:x[1], reverse= True)]#Sorts fusion_scores dictionary according to score which is mentioned as the key and loads the doc and score into a list of tuples\n",
        "\n",
        "  #4. Return the raranked list\n",
        "  return reranked_results\n",
        "\n",
        "#make chain\n",
        "retrieval_chain_rag_fusion= generate_queries | retriever.map() |reciprocal_rank_fusion\n",
        "docs= retrieval_chain_rag_fusion.invoke({\"question\":question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "WLQknhmsmUNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassThrough\n",
        "\n",
        "#RAG\n",
        "template= \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain= (\n",
        "    {\"context\":retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "finalrag_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "XQfCd-Wj0k5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}