{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9eTzrohOo42JbsksWvXAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeependraChaddha/RAG_Projects/blob/main/RAG_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, an environment will be set up then different method of query translation will be demnostrated. The methods to be deomnstrated are Multi-Query, RAG-Fusion, Decomposition, Step Back and HyDE"
      ],
      "metadata": {
        "id": "a4ZXDuINUz1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up Environment"
      ],
      "metadata": {
        "id": "YiqTqIwMVShJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Packages"
      ],
      "metadata": {
        "id": "c8jz5Q-BVZok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zgVNcB6TrKC"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Langsmith"
      ],
      "metadata": {
        "id": "4IcVnc2qWDHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "os.environ['LANGCHAIN_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY']=###YOUR OWN API KEY###"
      ],
      "metadata": {
        "id": "JB2H-rp2WFvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI api key"
      ],
      "metadata": {
        "id": "1gsra6fbWuyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=###your api key###"
      ],
      "metadata": {
        "id": "7zSgDc8jX_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Query"
      ],
      "metadata": {
        "id": "_5CaLqC1YLSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "c5t7CUCZY7Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### INDEXING #####\n",
        "#Loading document/ blog\n",
        "\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader=WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),       #link of blog\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\",\"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")###Made a WebBaseLoader instance in this line\n",
        "blog_docs=loader.load()#Used the WebBaseLoader instance to load the documents\n",
        "\n",
        "#SPLIT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")# made an instance of RecursiveCharacterTextSplitter\n",
        "###Make the Split###\n",
        "splits=text_splitter.split_documents(blog_docs)\n",
        "\n",
        "#Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore= Chroma.from_documents(documents=splits,\n",
        "                                   embedding=OpenAIEmbeddings())#Stored the split documents in Chroma vector database using OpenAIEmbeddings\n",
        "retriever=vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "wQMlx-FRYOp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt"
      ],
      "metadata": {
        "id": "vO_-tlLcfCiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#Multi-Query:Different Perspectives\n",
        "template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#Now the prompt will be broken down to generate multiple independent queries then these queries independently make the RAG retrieve documents, this way a more wide search can be done inside the documents\n",
        "from langchain_core.output_parsers import StrOutputParsers\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(\n",
        "    prompt_perspectives\n",
        "    | ChaOpenAI(temperature=0)\n",
        "    |StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")##This generates a list of queries"
      ],
      "metadata": {
        "id": "ENeLrRvafESU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "  \"\"\"This function makes a unique union of all the retrieved doucments\"\"\"\n",
        "  #Flatten list of lists, convert each document to string\n",
        "  flattened_docs=[dumps{docs} for sublist in documents for doc  in sublist]\n",
        "  #getting unique documents\n",
        "  unique_documents=list(set(flattened_docs))\n",
        "  return [loads(doc) for doc in unique_docs]\n",
        "#retrieve\n",
        "question= \"What is Task Decompositon for LLM agents?\"\n",
        "retrieval_chain=generate_queries |retriever.map()|get_unique_union #this gives the entire chain, first generate quesries then retrieves the different answers then get the unique union of all the documents retrieved\n",
        "docs= retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)\n"
      ],
      "metadata": {
        "id": "kfwdxkZ9hZUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}