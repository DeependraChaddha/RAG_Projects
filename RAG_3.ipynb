{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj72ZfDP/Y8LYDOe0jkeDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeependraChaddha/RAG_Projects/blob/main/RAG_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENVIRONMENT SETUP"
      ],
      "metadata": {
        "id": "xpGgBBOidBND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF3BDzFHcn9N"
      },
      "outputs": [],
      "source": [
        "#INSTALLING REQUIRED PACKAGES\n",
        "!pip install langchain_community tiktoken langchain_openai langchainhub chromadb langchain youtube-transcript-api pytub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SETTING LANGSMITH ENVIRONMENT\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]='true'\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]='https://api.smith.langchain.com'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=#YOUR_API_KEY\n"
      ],
      "metadata": {
        "id": "TnMoD8VBfg08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPENAI API KEY\n",
        "os.environ[\"OPENAI_API_KEY\"]=#YOUR_API_KEY"
      ],
      "metadata": {
        "id": "4WwME7rdf-Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ROUTING"
      ],
      "metadata": {
        "id": "YrXcYyVGgXtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGICAL ROUTING"
      ],
      "metadata": {
        "id": "W_5NkOaagk9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logical Routing inputs question into LLM to choose the correct database to use for embedding query"
      ],
      "metadata": {
        "id": "r6XdoLNOgxz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MAKING REQUIRED IMPORTS\n",
        "from typing import Literal\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#DataModel\n",
        "class RouteQuery(BaseModel): #Routes user query to most suitable datasource\n",
        "#datasource attribute/field can only take one of the 3 values\n",
        "  datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(#Field function used to add metadata or customize fields of models\n",
        "      ...,#This is to let PyDantic know that this field cannot be omitted\n",
        "                                                                      description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )#This class gives the format in which the llm will answer the query it is provided with in the RAG chain\n",
        "\n",
        "#LLM with function call\n",
        "llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n",
        "structured_llm=llm.with_structured_output(RouteQuery) #This asks the llm to provide answer in the structure/format specified by class defined above(inherited from BaseModel class of Pydantic)\n",
        "\n",
        "#Prompt\n",
        "system=\"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages([\n",
        "    (\"system\",system),\n",
        "    (\"human\",\"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Define Router\n",
        "router=prompt|structured_llm #This chain takes the prompt and pipes it to the llm which gives an output structured according to RouteQuery"
      ],
      "metadata": {
        "id": "QCL_3i1Rgm0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "#INVOKING THE ROUTER CHAIN\n",
        "result= router.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "Bzy7tOtKutKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "1_lsKGpwu8nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.datasource"
      ],
      "metadata": {
        "id": "D1bv2BQGu-0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_route(result):\n",
        "  if \"python_docs\" in result.datasource.lower():\n",
        "    return \"chain for python_docs\"\n",
        "  elif \"json_docs\" in result.datasource.lower():\n",
        "    return \"chain for json_docs\"\n",
        "  else:\n",
        "    return \"golang_docs\"\n",
        "\n",
        "from langchain.runnables import RunnableLambda\n",
        "#standardize function to be useful in Langchain pipeline architecture\n",
        "full_chain=router|RunnableLamda(choose_route)"
      ],
      "metadata": {
        "id": "oPyWJ5w6vURG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INVOKE FULL CHAIN\n",
        "full_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "w2EhFHTKwtVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEMANTIC ROUTING"
      ],
      "metadata": {
        "id": "bYLMWRnQw7Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Routing inputs and embeds the query and then selects suitable template to input in the LLM with the original question, the templates are made according to the problem to be solved."
      ],
      "metadata": {
        "id": "K3qtSefjhULE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MAKE ALL REQUIRED IMPORTS\n",
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "zp1urDsxw-03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 PROMPTS\n",
        "physics_template=\"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template=\"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "#Embed prompts\n",
        "\n",
        "#Specify embedding\n",
        "embeddings=OpenAIEmbeddings()\n",
        "\n",
        "#Give list of templates\n",
        "prompt_templates=[physics_template,math_template]\n",
        "\n",
        "#Make embeddings of all the templates\n",
        "prompt_embeddings=embeddings.embed_documents(prompt_templates)"
      ],
      "metadata": {
        "id": "t1LIZJ4CyFui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find prompt suitable for the question\n",
        "def prompt_router(input):\n",
        "  #1. This function takes the query as input returns the prompt more suitable for the question\n",
        "  #1.1. Embed Question\n",
        "  query_embedding=embeddings.embed_query(input[\"query\"])\n",
        "\n",
        "  #1.2. Compute Similarity\n",
        "  similarity=cosine_similarity([query_embeddings], prompt_embeddings)[0]\n",
        "  most_similar=prompt_templates[similarity.argmax()]\n",
        "\n",
        "  #1.3 Print which template is being used\n",
        "  print(\"Using Math Template\" if most_similar==math_template else \"Using Physics Template\")\n",
        "\n",
        "  #1.4 Return prompt template\n",
        "  return PromptTemplate.from_template(most_similar)"
      ],
      "metadata": {
        "id": "ukjkuUpkYazy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make entire chain\n",
        "chain=(\n",
        "    {\"query\":RunnablePassthrough()}\n",
        "    |RunnableLambda(prompt_router)\n",
        "    |ChatOpenAI()\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "#Invoke chain\n",
        "print(chain.invoke(\"What's a Black Hole?\"))"
      ],
      "metadata": {
        "id": "J2trwlIDc5_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}