{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQqZ7G6O9OIJDXcgn7mNGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeependraChaddha/RAG_Projects/blob/main/RAG_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SET UP ENVIRONMENT"
      ],
      "metadata": {
        "id": "2KxvjtI0PH60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PACKAGES"
      ],
      "metadata": {
        "id": "iY7Ebzz2PYXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML-X9U4TPAZX"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET UP LANGSMITH"
      ],
      "metadata": {
        "id": "nUs2ZUk9PrS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "os.environ['LANGCHAIN_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY']=##YOUR API KEY"
      ],
      "metadata": {
        "id": "bl_Dm-W-Pukz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPENAI API KEY"
      ],
      "metadata": {
        "id": "3UF7PdvLQLBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=##YOUR API KEY\n",
        "os.environ['COHERE_API_KEY']=##YOUR API KEY"
      ],
      "metadata": {
        "id": "qJAdrQV1QJff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RE-RANKING"
      ],
      "metadata": {
        "id": "QAWqwHtDVQjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RE RANKING RANKS AND FILTER/COMPRESS DOCUMENTS BASED ON RELEVANCE"
      ],
      "metadata": {
        "id": "ADlpjrELVYWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###INDEXING###\n",
        "\n",
        "#LOAD BLOG\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader=WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "        class_=(\"post-content\",\"post-title\", \"post-header\")\n",
        "      )\n",
        "    )\n",
        "  )\n",
        "blog_docs=loader.load()\n",
        "\n",
        "#SPLIT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "#MAKE SPLIT\n",
        "splits=text_splitter.split_documents(blog_docs)\n",
        "\n",
        "#INDEX\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "vectorstore=Chroma.from_documents(documents=splits,\n",
        "                                  #embedding=CohereEmbeddings()\n",
        "                                  embedding=OpenAIEmbeddings())\n",
        "#RETRIEVER\n",
        "retriever=vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "weiATEOWVUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "#RAG FUSION\n",
        "\n",
        "template=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "#MAKE PROMPT FROM TEMPLATE\n",
        "prompt_rag_fusion=Chat_Prompt_Template.from_template(template)"
      ],
      "metadata": {
        "id": "_Mw1boRUcnvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParsers\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries=(prompt_rag_fusion #make prompt\n",
        "                  | ChatOpenAI(temperature=0) #put through llm\n",
        "                  | StrOutputParsers()#parse output\n",
        "                  |(lambda x:x.split(\"\\n\")))#split output lines"
      ],
      "metadata": {
        "id": "Gj6W6cTLdHKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list],\n",
        "                           k=60):\n",
        "  #RECIPROCAL RANK FUSION SAME AS BEFORE\n",
        "  fused_scores={}\n",
        "\n",
        "  #2. Iterate through each list of documents\n",
        "  for docs in results:\n",
        "    #2.1. iterating through each document in a list\n",
        "    for rank, doc in enumerate(docs):\n",
        "      #2.1.1. convert each doc to string (assuming docs can be serialized to JSON)\n",
        "      doc_str=dumps(doc)\n",
        "      #2.1.2. check if doc_str in not already present in fused_scores(to avoid repitition), if its not, then add with initia score=0\n",
        "      if doc_str not in fused_scores:\n",
        "        fused_scores[doc_str]=0\n",
        "      #2.1.3. Retrieve the current score of the document(whether already present or just added)\n",
        "      previous_score=fused_scores[doc_str]\n",
        "      #2.1.4. Update score using RRF formula\n",
        "      fused_scores+= 1/(rank + k)\n",
        "  #3. Sort the documents based on rank in descending order\n",
        "  reranked_results=[(loads(doc),score) for doc,score in sorted(fusion_scores.items().key= lambda x:x[1], reverse= True)]#Sorts fusion_scores dictionary according to score which is mentioned as the key and loads the doc and score into a list of tuples\n",
        "\n",
        "  #4. Return the raranked list\n",
        "  return reranked_results\n",
        "\n",
        "question=\"What is task decomposition for LLM agents?\"\n",
        "\n",
        "#MAKING ENTIRE CHAIN\n",
        "retrieval_chain_rag_fusion=generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs=retrieval_chain_rag_fusion.invoke({\"question\":question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "OmXG2-mJfFu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassThrough\n",
        "\n",
        "#RAG\n",
        "template=\"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt= Chat_Prompt_Template.from_template(template)\n",
        "\n",
        "llm=ChatOpenAI(temperature=0)\n",
        "\n",
        "final_rag_chain=(\n",
        "    {\"context\":retrieval_chain_rag_fusion,\n",
        "     \"question\":itemgetter(\"question\")}\n",
        "    |prompt\n",
        "    |llm\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke(\n",
        "    {\"question\":question}\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "HXnNH072lcKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##USE COHERE RERANK"
      ],
      "metadata": {
        "id": "bSzTW_M_ow0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reranking, as the word suggests ranks the documents again after the first ranking, so that we get better ordering and better query results"
      ],
      "metadata": {
        "id": "PO8wZFw2ozde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ],
      "metadata": {
        "id": "gMbmdd7-o-He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "retriever =vectorstore.as_retriever(search_kwargs={\"k\":10})\n",
        "\n",
        "#RERANK\n",
        "compressor=CohereRerank()\n",
        "compression_retriever=ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                      base_retriever=retriever)\n",
        "compressed_docs=compression_retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "id": "Yx7bRe-tpRYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}